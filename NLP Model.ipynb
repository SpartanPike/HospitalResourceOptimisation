{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0AHRtDEPLam",
        "outputId": "e25a1530-403b-4d71-e91b-1b6ab3fa11d5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.37.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.0.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.0.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.5)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.10)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.2->gradio) (11.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets pandas scikit-learn gradio transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"dmacres/mimiciii-hospitalcourse-meta\")"
      ],
      "metadata": {
        "id": "G3zr64i3PNpq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code below"
      ],
      "metadata": {
        "id": "BRzfQknA9pPm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import re\n",
        "import gradio as gr\n",
        "\n",
        "# Function to extract doctor types from text\n",
        "def extract_doctor_types(text):\n",
        "    # Define a list of doctor types to search for\n",
        "    doctor_types = [\n",
        "        \"neurosurgeon\", \"cardiologist\", \"hematologist\", \"oncologist\",\n",
        "        \"radiologist\", \"endocrinologist\", \"gastroenterologist\", \"urologist\",\n",
        "        \"orthopedic\", \"dermatologist\", \"psychiatrist\", \"pulmonologist\",\n",
        "        \"neurologist\", \"ophthalmologist\", \"ENT\", \"rheumatologist\"\n",
        "    ]\n",
        "    found_doctors = [doc for doc in doctor_types if re.search(doc, text, re.IGNORECASE)]\n",
        "    return found_doctors\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"dmacres/mimiciii-hospitalcourse-meta\")\n",
        "# Convert the dataset to a pandas DataFrame\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Extract doctor types from the target text\n",
        "df['doctor_types'] = df['target_text'].apply(extract_doctor_types)\n",
        "\n",
        "# Filter out rows without any doctor types identified\n",
        "df = df[df['doctor_types'].apply(len) > 0]\n",
        "\n",
        "# Define features (X) and labels (y)\n",
        "X = df['extractive_notes_summ']\n",
        "y = df['doctor_types']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Binarize the target labels for multi-label classification\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_bin = mlb.fit_transform(y_train)\n",
        "y_test_bin = mlb.transform(y_test)\n",
        "\n",
        "# Train a OneVsRest Logistic Regression model\n",
        "model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
        "model.fit(X_train_vec, y_train_bin)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_bin = model.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_bin, y_pred_bin)\n",
        "f1 = f1_score(y_test_bin, y_pred_bin, average='micro')\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
        "\n",
        "# Function for Gradio to make predictions on user input\n",
        "def predict_doctor_types(text):\n",
        "    text_vec = vectorizer.transform([text])\n",
        "    pred_bin = model.predict(text_vec)\n",
        "    pred_labels = mlb.inverse_transform(pred_bin)\n",
        "    return pred_labels[0] if pred_labels else []\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_doctor_types,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter medical note here...\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Doctor Types\"),\n",
        "    title=\"Doctor Type Prediction from Medical Notes\",\n",
        "    description=\"Enter medical notes to predict the types of doctors involved.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "JptzkZk9Na-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "2a6fcc89-8a99-4b3c-ecc1-214093d01f31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 87.90%\n",
            "F1 Score: 93.44%\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://2a190e4db1d74852c8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2a190e4db1d74852c8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Check the distribution of doctor types in the dataset\n",
        "doctor_type_counts = Counter([doc for sublist in df['doctor_types'] for doc in sublist])\n",
        "print(doctor_type_counts)\n"
      ],
      "metadata": {
        "id": "kZe5kfqDtxFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7192e5e-2ec0-4ce8-8755-eed194a213bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'ENT': 23539, 'cardiologist': 890, 'orthopedic': 791, 'oncologist': 392, 'urologist': 254, 'neurologist': 185, 'psychiatrist': 180, 'pulmonologist': 151, 'gastroenterologist': 104, 'radiologist': 85, 'hematologist': 75, 'endocrinologist': 74, 'neurosurgeon': 37, 'ophthalmologist': 28, 'rheumatologist': 24, 'dermatologist': 12})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import re\n",
        "import string\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.sparse import vstack\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract doctor types from text\n",
        "def extract_doctor_types(text):\n",
        "    doctor_types = [\n",
        "        \"neurosurgeon\", \"cardiologist\", \"hematologist\", \"oncologist\",\n",
        "        \"radiologist\", \"endocrinologist\", \"gastroenterologist\", \"urologist\",\n",
        "        \"orthopedic\", \"dermatologist\", \"psychiatrist\", \"pulmonologist\",\n",
        "        \"neurologist\", \"ophthalmologist\", \"ENT\", \"rheumatologist\"\n",
        "    ]\n",
        "    found_doctors = [doc for doc in doctor_types if re.search(doc, text, re.IGNORECASE)]\n",
        "    return found_doctors\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"dmacres/mimiciii-hospitalcourse-meta\")\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Extract doctor types from the target text\n",
        "df['doctor_types'] = df['target_text'].apply(extract_doctor_types)\n",
        "\n",
        "# Filter out rows without any doctor types identified\n",
        "df = df[df['doctor_types'].apply(len) > 0]\n",
        "\n",
        "# Preprocess the text data\n",
        "df['extractive_notes_summ'] = df['extractive_notes_summ'].apply(preprocess_text)\n",
        "\n",
        "# Define features (X) and labels (y)\n",
        "X = df['extractive_notes_summ']\n",
        "y = df['doctor_types']\n",
        "\n",
        "# Check for class imbalance\n",
        "doctor_type_counts = Counter([doc for sublist in df['doctor_types'] for doc in sublist])\n",
        "print(doctor_type_counts)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Binarize the target labels for multi-label classification\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_bin = mlb.fit_transform(y_train)\n",
        "y_test_bin = mlb.transform(y_test)\n",
        "\n",
        "# Apply SMOTE to each label individually and stack results\n",
        "smote = SMOTE()\n",
        "\n",
        "# Initialize empty arrays for resampled data\n",
        "X_resampled = None\n",
        "y_resampled = None\n",
        "\n",
        "for i in range(y_train_bin.shape[1]):\n",
        "    X_res, y_res = smote.fit_resample(X_train_vec, y_train_bin[:, i].reshape(-1, 1))\n",
        "    if X_resampled is None:\n",
        "        X_resampled = X_res\n",
        "        y_resampled = y_res\n",
        "    else:\n",
        "        X_resampled = vstack((X_resampled, X_res))\n",
        "        y_resampled = np.hstack((y_resampled, y_res))\n",
        "\n",
        "# Train a OneVsRest Logistic Regression model\n",
        "model = OneVsRestClassifier(LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Function for Gradio to make predictions on user input\n",
        "def predict_doctor_types(text):\n",
        "    text = preprocess_text(text)\n",
        "    text_vec = vectorizer.transform([text])\n",
        "    pred_bin = model.predict(text_vec)\n",
        "    pred_labels = mlb.inverse_transform(pred_bin)\n",
        "    return pred_labels[0] if pred_labels else []\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_doctor_types,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter medical note here...\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Doctor Types\"),\n",
        "    title=\"Doctor Type Prediction from Medical Notes\",\n",
        "    description=\"Enter medical notes to predict the types of doctors involved.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "qaN5FsNkhyRP",
        "outputId": "5822ad65-48c5-44f1-f4d8-f9a1f9fc5076"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'ENT': 23539, 'cardiologist': 890, 'orthopedic': 791, 'oncologist': 392, 'urologist': 254, 'neurologist': 185, 'psychiatrist': 180, 'pulmonologist': 151, 'gastroenterologist': 104, 'radiologist': 85, 'hematologist': 75, 'endocrinologist': 74, 'neurosurgeon': 37, 'ophthalmologist': 28, 'rheumatologist': 24, 'dermatologist': 12})\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://9b4405c7d132d5effa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9b4405c7d132d5effa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install smote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qc39NWb0j8R",
        "outputId": "523f844b-0170-4033-85d2-b2a8fa039c32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting smote\n",
            "  Downloading smote-0.1-py2.py3-none-any.whl (3.3 kB)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from smote) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from smote) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->smote) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->smote) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->smote) (3.5.0)\n",
            "Installing collected packages: smote\n",
            "Successfully installed smote-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your model setup and data loading are correct up to this point\n",
        "# Predict probabilities (if using OneVsRestClassifier with LogisticRegression)\n",
        "y_pred_prob = model.predict_proba(X_test_vec)\n",
        "\n",
        "# Check the shape and content of y_pred_prob\n",
        "print(f\"Shape of y_pred_prob: {y_pred_prob.shape}\")\n",
        "print(f\"Example of y_pred_prob: {y_pred_prob[:5]}\")\n",
        "\n",
        "# Assuming y_pred_prob has shape (n_samples, n_classes)\n",
        "# Print the unique values in y_pred_prob to check the range of probabilities\n",
        "print(f\"Unique values in y_pred_prob: {np.unique(y_pred_prob)}\")\n",
        "\n",
        "# Troubleshoot why y_pred_prob has only 2 columns instead of 16\n",
        "# Ensure that your model setup and data preprocessing correctly handle all 16 classes\n",
        "\n",
        "# If y_pred_prob has shape (4712, 2), debug further:\n",
        "# - Check model configuration (OneVsRestClassifier with LogisticRegression)\n",
        "# - Verify data preprocessing (encoding of target labels)\n",
        "\n",
        "# Once y_pred_prob is correctly predicting probabilities for 16 classes,\n",
        "# proceed with thresholding and evaluation as previously discussed.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1R8-8mX004b",
        "outputId": "879b0f8f-8756-4fd0-ba6c-3bebea079b8e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_pred_prob: (4712, 2)\n",
            "Example of y_pred_prob: [[0.89412235 0.10587765]\n",
            " [0.93387798 0.06612202]\n",
            " [0.50207605 0.49792395]\n",
            " [0.61418695 0.38581305]\n",
            " [0.92929515 0.07070485]]\n",
            "Unique values in y_pred_prob: [2.34455340e-04 3.53626396e-04 5.70811099e-04 ... 9.99429189e-01\n",
            " 9.99646374e-01 9.99765545e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "ufnRSlkRK_fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code 3"
      ],
      "metadata": {
        "id": "l3smGQPGKhNW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import re\n",
        "import string\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "# Function to extract doctor types from text\n",
        "def extract_doctor_types(text):\n",
        "    doctor_types = [\n",
        "        \"neurosurgeon\", \"cardiologist\", \"hematologist\", \"oncologist\",\n",
        "        \"radiologist\", \"endocrinologist\", \"gastroenterologist\", \"urologist\",\n",
        "        \"orthopedic\", \"dermatologist\", \"psychiatrist\", \"pulmonologist\",\n",
        "        \"neurologist\", \"ophthalmologist\", \"ENT\", \"rheumatologist\"\n",
        "    ]\n",
        "    found_doctors = [doc for doc in doctor_types if re.search(doc, text, re.IGNORECASE)]\n",
        "    return found_doctors\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"dmacres/mimiciii-hospitalcourse-meta\")\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Extract doctor types from the target text\n",
        "df['doctor_types'] = df['target_text'].apply(extract_doctor_types)\n",
        "\n",
        "# Filter out rows without any doctor types identified\n",
        "df = df[df['doctor_types'].apply(len) > 0]\n",
        "\n",
        "# Preprocess the text data\n",
        "df['extractive_notes_summ'] = df['extractive_notes_summ'].apply(preprocess_text)\n",
        "\n",
        "# Define features (X) and labels (y)\n",
        "X = df['extractive_notes_summ']\n",
        "y = df['doctor_types']\n",
        "\n",
        "# Check for class imbalance\n",
        "doctor_type_counts = Counter([doc for sublist in df['doctor_types'] for doc in sublist])\n",
        "print(doctor_type_counts)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Binarize the target labels for multi-label classification\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_bin = mlb.fit_transform(y_train)\n",
        "y_test_bin = mlb.transform(y_test)\n",
        "\n",
        "# Apply SMOTE to handle class imbalance (optional)\n",
        "# SMOTE is typically applied to the entire multi-label set, not each label individually\n",
        "# smote = SMOTE()\n",
        "# X_train_res, y_train_res = smote.fit_resample(X_train_vec, y_train_bin)\n",
        "\n",
        "# Train a OneVsRest Logistic Regression model\n",
        "model = OneVsRestClassifier(LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "model.fit(X_train_vec, y_train_bin)\n",
        "\n",
        "# Function for Gradio to make predictions on user input\n",
        "def predict_doctor_types(text):\n",
        "    text = preprocess_text(text)\n",
        "    text_vec = vectorizer.transform([text])\n",
        "    pred_bin = model.predict(text_vec)\n",
        "    pred_labels = mlb.inverse_transform(pred_bin)\n",
        "    return pred_labels[0] if pred_labels else []\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict_doctor_types,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter medical note here...\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Doctor Types\"),\n",
        "    title=\"Doctor Type Prediction from Medical Notes\",\n",
        "    description=\"Enter medical notes to predict the types of doctors involved.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "74U7PaBeL4o4",
        "outputId": "51c7941b-db87-4b7d-8a42-331e46e0acfa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'ENT': 23539, 'cardiologist': 890, 'orthopedic': 791, 'oncologist': 392, 'urologist': 254, 'neurologist': 185, 'psychiatrist': 180, 'pulmonologist': 151, 'gastroenterologist': 104, 'radiologist': 85, 'hematologist': 75, 'endocrinologist': 74, 'neurosurgeon': 37, 'ophthalmologist': 28, 'rheumatologist': 24, 'dermatologist': 12})\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://adceb9bd474a1a6f8f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://adceb9bd474a1a6f8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7bpzeC_L5BX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
